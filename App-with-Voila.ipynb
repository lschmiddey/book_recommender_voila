{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasses Buchempfehlungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Was würde Lasse zu diesem Buch sagen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nimm ein Bild von einer Seite im Buch auf, lade es hier hoch und du wirst erfahren, wie gut Lasse dieses Buch finden würde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voila\n",
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import *\n",
    "from PIL import Image, ImageFilter \n",
    "import pytesseract\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling: voila\n",
      "- Writing config: /opt/conda/envs/fastai/etc/jupyter\n",
      "    - Validating...\n",
      "      voila 0.2.3 \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter serverextension enable voila --sys-prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1kk_SvwpwZeuLnZirW5vbrd8FEnm7yJRt',\n",
    "                                    dest_path='./export.pkl',\n",
    "                                    unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_img(input_img):\n",
    "    \n",
    "    img = input_img.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None)\n",
    "    img = img.filter(ImageFilter.MedianFilter)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessdata_dir_config = \"/usr/share/tesseract-ocr/4.00/tessdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(img):\n",
    "    return pytesseract.image_to_string(img, lang=\"deu\", config=tessdata_dir_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_pattern(text):\n",
    "    return pattern.sub(lambda m: rep[re.escape(m.group(0))], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = {\"\\n\": \"\", \"`\": \"\", '%':\"\", '°': '', '&':'', '‘':'', '€':'e', '®':'', '\\\\': '', '5':'s', '1':'i', '_':'', '-':''} # define desired replacements here\n",
    "\n",
    "# use these three lines to do the replacement\n",
    "rep = dict((re.escape(k), v) for k, v in rep.items()) \n",
    "#Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions\n",
    "pattern = re.compile(\"|\".join(rep.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "def tokenize_text(sent):\n",
    "    \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        truncation=True,\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #padding='longest',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    # Add the encoded sentence to the list.    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text):\n",
    "    \n",
    "    input_ids, attention_masks = tokenize_text(text)\n",
    "    dataset = TensorDataset(input_ids, attention_masks)\n",
    "    batch_size = 1\n",
    "    app_dataloader = DataLoader(\n",
    "                dataset, # The validation samples.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    return app_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataloader):\n",
    "    # Prediction on test set\n",
    "    device = torch.device('cpu')\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    predictions = []\n",
    "\n",
    "    # Predict \n",
    "    for batch in dataloader:\n",
    "\n",
    "            # Add batch to CPU\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask = batch\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and \n",
    "            # speeding up prediction\n",
    "            with torch.no_grad():\n",
    "              # Forward pass, calculate logit predictions\n",
    "              outputs = model(b_input_ids, token_type_ids=None, \n",
    "                              attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            predictions.append(logits)\n",
    "            \n",
    "            return np.argmax(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-german-cased'\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', PRE_TRAINED_MODEL_NAME)    # Download vocabulary from S3 and cache.\n",
    "n_classes=5\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-german-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = n_classes, # The number of output labels--2 for binary classification.\n",
    "                    # You can increase this for multi-class tasks.   \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model.load_state_dict(torch.load(p/'export.pkl', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_upload = widgets.FileUpload()\n",
    "out_pl = widgets.Output()\n",
    "rating_widget = widgets.Label()\n",
    "btn_run = widgets.Button(description='Lasses Empfehlung:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_text(change):\n",
    "    img = PILImage.create(btn_upload.data[-1])\n",
    "    out_pl.clear_output()\n",
    "    with out_pl: display(proc_img(img).to_thumb(256,256))\n",
    "    text = use_pattern(get_text(proc_img(img)))\n",
    "    star_rating = predict(create_dataloader(text))\n",
    "    rating_widget.value = f'Lasse würde diesem Buch {star_rating+1} Stern(e) von 5 Sternen geben!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_run.on_click(on_click_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8e5b6ae1034c38af1da451c9c8fee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Upload Bild von Buchseite'), FileUpload(value={}, description='Upload'), Button(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox([widgets.Label('Upload Bild von Buchseite'),\n",
    "     btn_upload, btn_run, out_pl, rating_widget])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
